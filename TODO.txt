TODO:
[ ] add database
[ ] support requests/second and delays from robots.txt
[ ] save sitemap content to db
[ ] store links in a postgres database

DOING:
[X] skip links with javascript:void(0) in src
[ ] handle onClick events
[ ] parse other files like .pdf, .doc, .docx, .ppt, and .pptx
[ ] respect craw-delay in robots.txt

DONE:
[X] get all links from the FRI page
[X] get all images from the FRI page
[X] read robots.txt before crawling a site
[X] store info if a site has robots file in robots array
[X] parse robots.txt for sitemaps
[X] parse sitemaps
[X] use urllib.parse.urlparse to canonicalize URLs