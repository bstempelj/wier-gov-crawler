TODO:
[x] add database
[ ] support requests/second and delays from robots.txt
[x] save sitemap content to db
[x] store links in a postgres database
[ ] save actual data to database (imgs, pdf, doc)

DOING:
[X] skip links with javascript:void(0) in src
[ ] handle onClick events
[ ] parse other files like .pdf, .doc, .docx, .ppt, and .pptx
[ ] respect craw-delay in robots.txt

DONE:
[X] get all links from the FRI page
[X] get all images from the FRI page
[X] read robots.txt before crawling a site
[X] store info if a site has robots file in robots array
[X] parse robots.txt for sitemaps
[X] parse sitemaps
[X] use urllib.parse.urlparse to canonicalize URLs


if you have trouble with ssl certificate -> sudo pip3 install certifi && https://stackoverflow.com/questions/27835619/urllib-and-ssl-certificate-verify-failed-error